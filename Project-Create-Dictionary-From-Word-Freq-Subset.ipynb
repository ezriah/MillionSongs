{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility file that creates a dictionary of tracks (key) and lyrics (value) for use with text mining algorithms. The output is a json file, \"lyrics_dict_subset.json\", which can be used in other modules. \n",
    "\n",
    "This particular version only has the lyrics that are in common with the 10k song subset. For a more generalized version use the file \"Create-Dictionary-From-Word-Freq.ipynb\"\n",
    "\n",
    "We first import the three modules we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create connections to our database. We'll be connecting to the [mxm_dataset.db](http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/mxm_dataset.db), which has lyrics for many of the tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_lyrics = sqlite3.connect('../Data/mxm_dataset.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the mxm_dataset.db, we want a list of unique tracks. We find lyrics for 237,662 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237662"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = pd.read_sql(\"SELECT DISTINCT track_id FROM lyrics\", con = conn_lyrics)\n",
    "len(tracks) # 237662 tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import a file we created earlier from the subset, which includes the track_id. We'll use this to restrict our cluster analysis to those songs that are in both the lyrics database and the subset. \n",
    "\n",
    "After importing we drop the first two characters (b'), and the last character('), which are artifacts of the encoding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_load_path = '../Data/MillionSongSubset/data'\n",
    "project_df = pd.read_pickle(save_load_path+'/project_df.pkl')\n",
    "track_id = []\n",
    "track_id = project_df['track_id'].map(lambda x: str(x)[2:len(x)+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We turn this into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "track_id =pd.DataFrame(track_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we merge the tracks that have lyrics available with the tracks in the subset. Since we're only interested in the tracks for which there are lyrics, we use an 'inner' join, which uses an intersection of keys from both dataframes. To learn more about different types of merges: http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebra . \n",
    "\n",
    "We find that there are 2350 such tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracks = track_id.merge(tracks, how='inner', on='track_id')\n",
    "len(df_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the tracks from the dataframe to pull the lyrics from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set this to the number of tracks you want to pull from db\n",
    "num_tracks = len(df_tracks['track_id']) \n",
    "# intialize empty dictionary to store tracks and lyrics\n",
    "my_dict = {}\n",
    "\n",
    "for i in range(0,num_tracks): \n",
    "    # assign the value of the track at current index to current track\n",
    "    current_track = df_tracks.track_id[i]\n",
    "\n",
    "    # pull the lyrics for that track and store it in a list\n",
    "    res = conn_lyrics.execute(\"SELECT word, count FROM lyrics WHERE track_id = ?\", [current_track])\n",
    "    results = res.fetchall()\n",
    "\n",
    "    # multiply the word by the number of times it occurs for each word in list\n",
    "    li = [(x[0] + ' ') * x[1] for x in results]\n",
    "    \n",
    "    # use this version to get a single copy of each word\n",
    "    # li = [x[0]for x in results]\n",
    "\n",
    "    # get rid of commas between words\n",
    "    li = str(li).replace(',','')\n",
    "    \n",
    "    # get rid of quotes between words\n",
    "    li = str(li).replace(\"'\",'')\n",
    "    \n",
    "    # add track and lyrics to dictionary\n",
    "    my_dict[current_track] = li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We verify the number of tracks processed, and take a look at the output of my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the data to a json file. Why json? A dictionary stores data in the same way json stores data, so it seemed appropriate to use. It's human readable. We can open a json file in notepad and see our data. It's also faster than a pickle file, as benchmarked [here](https://kovshenin.com/2010/pickle-vs-json-which-is-faster/) . \n",
    "\n",
    "We've also included the code to save the data to a pickle file, in case those reasons aren't compelling enough to overcome a preference for pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to json file in same directory\n",
    "import json\n",
    "with open('lyrics_dict.json', 'w') as fp:\n",
    "    # arguments can include indent=n or None, sort_keys = True\n",
    "    json.dump(my_dict, fp, indent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dictionary to pickle\n",
    "#import pickle\n",
    "#with open('lyrics_dict.p', 'wb') as fp:\n",
    "#    pickle.dump(my_dict, fp)\n",
    "    \n",
    "#with open('lyrics_dict.p', 'rb') as fp:\n",
    "#    data_pickle = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_lyrics.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
