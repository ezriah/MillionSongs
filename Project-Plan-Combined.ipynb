{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation: We are going to use the bag of words and the frequency of words from the Million Song Dataset lyrics to perform Sentiment analysis. We will then use network analysis to create a network of songs with common words. We may also characterize the moods using acoustic information (loudness, pitch, timbre)  or artist location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Techniques we plan to use and how we plan to use them\n",
    "\n",
    "\n",
    "We are going to use NLTK (Natural Language Toolkit) to find trends in the data, highest frequency of words being repeated. \n",
    "\n",
    "We think it would be useful to compare multiple techniques and move forward using the one that did the best job classifying the texts as positive, negative and neutral sentiment. Here is a list of resources for each technique we may include in our investigation:\n",
    "\n",
    "* Naive Bayes\n",
    "    * [Python Data Science Cookbook](https://www.packtpub.com/packtlib/book/Big-Data-and-Business-Intelligence/9781784396404/6/ch06lvl1sec70/Classifying%20documents%20using%20Nave%20Bayes)\n",
    "    * [Python Data Analysis](https://www.packtpub.com/packtlib/book/Big-Data-and-Business-Intelligence/9781783553358/9/ch09lvl1sec96/Naive%20Bayes%20classification)\n",
    "    * [Python 3 Text Processing with NLTK Cookbook](https://www.packtpub.com/packtlib/book/Big-Data-and-Business-Intelligence/9781783553358/9/ch09lvl1sec96/Naive%20Bayes%20classification)\n",
    "\n",
    "* Maximum Entropy Classifier\n",
    "    * [Python 3 Text Processing with NLTK Cookbook](https://www.packtpub.com/packtlib/book/Application-Development/9781782167853/7/ch07lvl1sec79/Training%20a%20maximum%20entropy%20classifier)\n",
    "    \n",
    "* Support Vector Machines\n",
    "    * http://marcobonzanini.com/2015/01/19/sentiment-analysis-with-python-and-scikit-learn/\n",
    "\n",
    "* Decision Tree\n",
    "    * [Python 3 Text Processing with NLTK Cookbook](https://www.packtpub.com/packtlib/book/Application-Development/9781782167853/7/ch07lvl1sec78/Training%20a%20decision%20tree%20classifier)\n",
    "    \n",
    "Also see: \n",
    "http://streamhacker.com/?s=sentiment+analysis\n",
    "\n",
    "http://andybromberg.com/sentiment-analysis-python/\n",
    "    \n",
    "Metrics used to evaluate classifiers: [Start here](https://www.packtpub.com/packtlib/book/Application-Development/9781782167853/7/ch07lvl1sec81/Measuring%20precision%20and%20recall%20of%20a%20classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What data we need to acquire from the dataset\n",
    "\n",
    "There are two options for data acquisition:\n",
    "\n",
    "We can process the files as individual .txt files. In order to process all the 210,000+ files, we would use AWS EC2 instances, Spark and PySpark to perform analysis.\n",
    "\n",
    "Alternatively, we can pull the track_id, word, and count columns from the mxm_dataset.db database. \n",
    "\n",
    "Further we would need a dictonary words which have known sentiments to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How we will prepare our data for analysis\n",
    "\n",
    "After pulling the data from the dataset, we'll need to format it as needed for each algorithm. Right now it's set up as a frequency count. \n",
    "\n",
    "For NLTK: [NLTK classifiers expect dictionaries](https://www.packtpub.com/packtlib/book/Application-Development/9781782167853/7/ch07lvl1sec76/Bag%20of%20words%20feature%20extraction). We'll have to process it so it fits the format expected. It's also possible that different techniques require different formats, in which case we'll have to process the data to fit the expected format. I don't have more details at this point, given the short suspense of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Rap song idea\n",
    "\n",
    "I had a conversation about six months ago about the change in rap music over time. In the 80's and early 90's Rap music was political. It was about social progress. Fast forward to today, and it's about social status, money, women, etc. Using a subset of lyrics with the Rap (or similar) tag, can we create topic categories using cluster analysis, look at the words in each cluster as our source files for sentiment, and then classify congs using that? Then maybe look at the song year and plot them to see if our hypothesis seems valid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Use Network Analysis for songs on common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation\n",
    "We are going to use *Network X* to create networks on songs with common words and track ids as uniques identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data needed from MSD\n",
    "We will need the lyrics and the bag of words of all the tracks to perform the processing. Further we would need a dictonary words which have known sentiments to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will have to seperate each track by the **Track IDs** and fill in the words as per their respective frequencies. The data will be stored in individual track .txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Mood Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Techniques we plan to use and how we plan to use them\n",
    "\n",
    "Explore data visualization techniques to characterize the moods. This needs to be thought through a lot more. An understanding of the variables we investigate will clarify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What data we need to acquire from the dataset\n",
    "\n",
    "Some interesting variables to include might be:\n",
    "\n",
    "* duration\n",
    "* key/key_confidence\n",
    "* mode/mode_confidence\n",
    "* loudness (maybe)\n",
    "* tempo\n",
    "\n",
    "* artist_latitude/artist_longitude: these could be interesting to look at, especially if we look at sentiment that involves more than just positive/negative, and especially if we use a specific genre subset in which there are known regional differences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How we will prepare our data for analysis\n",
    "\n",
    "Good question. It depends on the data. If the data is an array some of the questions in the following section will apply. Need to figure out more here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "\n",
    "* Which acoustic information do we want to include?\n",
    "* How much of each acoustic information do we want to include?\n",
    "    * How do we sample the acoustic information? Is this just first x pieces of multipiece info or is there a better way to do it?\n",
    "* Do we want to use the whole dataset or a subset (based on genre maybe?)\n",
    "\n",
    "\n",
    "Additional References\n",
    "\n",
    "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n",
    "[Sentiment analysis algorithms and applications: A Survey]( http://www.sciencedirect.com/science/article/pii/S2090447914000550)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
